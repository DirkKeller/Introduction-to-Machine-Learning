{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2021, Utrecht University*\n",
    "\n",
    "Total points: 9 + 1 bonus (+ 1 for free)\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coin Flipping\n",
    "In this second assignment, you're going to see how some of the central concepts from Bayesian machine learning behave in different scenarios. You'll be looking at the coin flipping example that has also taken a central place in the book and the lectures so far.\n",
    "\n",
    "When flipping a coin $N$ times, under very reasonable assumptions, the probability of getting $y$ times heads is given by the binomial distribution with parameters $N$ and $r$, where $r$ is the probability that the coin lands heads on one flip.\n",
    "\n",
    "SciPy includes functions for working with many well-known distributions, allowing you to sample from them and compute probabilities and densities, as well as many other properties. For the binomial distribution, you can compute the probability of getting $y$ heads with `binom.pmf(y, N, r)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** (1 point): For the values $r = 0.0, 0.01, \\ldots, 1.0$, compute the likelihood of seeing 9 out of 10 heads, and plot these in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import beta as beta_dist\n",
    "import scipy.special as sps\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnnElEQVR4nO3deZycVZ3v8c+v9yW9pbuzr0AWwhJImgACCioYUAwISkDBizgZrjJeveMdmbvI3HE2ZhyXmUEyGWS86iioEIwSWdRBQAikQ0JIAgmdhKSXLJ21k3Q6vdTv/lHVWOl0p59Oquqpqv6+X69+ddXznFPP76SbH6fPc55zzN0REZHslRN2ACIiklxK9CIiWU6JXkQkyynRi4hkOSV6EZEsp0QvIpLl8oIUMrP5wLeBXOAhd/+7AcpdBKwAbnH3nw2lbryamhqfMmVKoAaIiAisWrVqj7vX9ndu0ERvZrnAA8DVQBOw0syWufuGfsrdDzw91Lp9TZkyhfr6+sFCExGRGDPbNtC5IEM384AGd9/i7p3AI8CCfsr9CfAYsPsU6oqISJIESfTjgca4902xY+8ys/HAjcDiodYVEZHkCpLorZ9jfddN+BbwFXfvOYW60YJmi8ys3szqW1tbA4QlIiJBBLkZ2wRMjHs/AWjpU6YOeMTMAGqA68ysO2BdANx9CbAEoK6uTgvwiIgkSJBEvxKYZmZTgWZgIXBbfAF3n9r72sy+B/zS3Z8ws7zB6oqISHINmujdvdvM7iE6myYXeNjd15vZ3bHzfcflB62bmNBFRCQIS8dliuvq6lzTK0VEgjOzVe5e19+5QA9MiYhku7aOLn63sZVte49QVpTPZWfVcNaoEWGHlRBK9CIyrHX3RPjX57fw4HObOXys+7hzHz5vLH91w7lUlRaEFF1iKNGLyLB1oL2Tu3+4ihVb9nH1rNHc/b4zOGdcBXuPdPKTlY08+Nxm1rUc5NFFlzKmoijscE+ZFjUTkWHp4NEuPvXdV3ht+wG+8YnZ/NsddcydPJKi/FzGVxbzpaun8+NFF7P3cCeffvhV2ju7B//QNKVELyLDTmd3hLu+t5KNOw/xr7fP5WNzJvRbbu7kkTz4qTls2n2Iv37yzRRHmThK9CIy7PzVkxuo37afb3ziAq6aMeqkZa+YVsuiK87gP17ZzksNe1IUYWIp0YvIsLL8jR18/+Vt/NEVU7l+9rhAdb509XTGVxbz18vfJBJJvynpg1GiF5FhY9+RTv7PE+s4f0IFfzZ/ZuB6Rfm5/I8PzWB9Sxu/WNvvKi5pTYleRIaNv/zFeto6uvj7m88nP3do6e+js8dx1qgRLHl+C+n4oOnJKNGLyLDw4tt7eGJNC5+78ixmjikfcv2cHOOzl09lfUsbL2/Zm4QIk0eJXkSyXk/E+dovNzBxZDGfu+rMU/6cGy4cT3VpAd/7/TuJCy4FlOhFJOs9urKRjbsO8efXnk1hXu4pf05Rfi43zZ3Ab9/azd7DxxIYYXIp0YtIVjtyrJtvPLuReVNGcu25Y077826aM4HuiLPs9cy5KatELyJZ7QcrtrHncCf3XjeT2OZIp2XGmDLOHV/O4681JyC61FCiF5GsdeRYN0ue38L7ptcyZ1JVwj73o7PH8UbzQRr3tSfsM5NJiV5Estb3X97GviOdfPGD0xL6uR86JzoE9MyGXQn93GRRoheRrBTtzW/myhm1XJjA3jzA5OpSZo4p4+n1OxP6uckSKNGb2Xwz22hmDWZ2bz/nF5jZWjNbY2b1ZnZ53Ll3zOyN3nOJDF5EZCA/qW9kf3sXf/L+xPbme10zazT17+xj35HOpHx+Ig2a6M0sF3gAuBaYBdxqZrP6FPsNMNvdLwA+AzzU5/xV7n7BQNtciYgkUndPhO++uJWLplQxd3Jie/O9rpw5iojDS5vTf6GzID36eUCDu29x907gEWBBfAF3P+x/eCa4FMis54NFJKs8tX4nTfuP8kdXnJG0a5w/voKyojxefDs7Ev14oDHufVPs2HHM7EYzewt4kmivvpcDz5jZKjNbNNBFzGxRbNinvrW1NVj0IiJ9uDv/9vwWptaU8sGzRyftOnm5OVx6RjUvvL0n7de+CZLo+5t4ekKr3H2pu88EbgC+FnfqMnefQ3To5/Nm9t7+LuLuS9y9zt3ramtrA4QlInKile/s5/Wmg9x1+VRyck5/3vzJXD6thuYDR9me5tMsgyT6JmBi3PsJwICPhLn788CZZlYTe98S+74bWEp0KEhEJCm+++IWqkryuWmAXaMS6fKzagB4Ic2Hb4Ik+pXANDObamYFwEJgWXwBMzvLYo+cmdkcoADYa2alZlYWO14KXAOsS2QDRER67Th4lGc37GLhvEkUF5z6mjZBTa0pZWxFES9vTu/VLPMGK+Du3WZ2D/A0kAs87O7rzezu2PnFwE3AHWbWBRwFbnF3N7PRwNLY/wPygB+5+1NJaouIDHM/fmU7Dtw2b1JKrmdm1E0ZSf07+1JyvVM1aKIHcPflwPI+xxbHvb4fuL+feluA2acZo4jIoLp6Ivx4ZSNXzRjFxJElKbvu3EmV/OL1FloOHGVcZXHKrjsUejJWRLLCM+t30XroGLdfMjml150Tm6f/2vb9Kb3uUCjRi0hW+MGKd5g4spj3Tk/trL2zx5ZTlJ/Dqm1K9CIiSfP2rkOs2LKPT148mdwkT6nsKz83h9kTKnlNiV5EJHn+45XtFOTl8Im6iYMXToK5k6tY39JGR1dPKNcfjBK9iGS0jq4elq5uZv45YxhZWhBKDBdMrKQ74qxvORjK9QejRC8iGe2ZDbs4eLSLWy4KpzcPcO74CgDWt7SFFsPJKNGLSEb7aX0j4yuLufSM6tBiGFtRxMjSAtY3K9GLiCRU0/52XmzYw8frJiR9XZuTMTPOGVfOOg3diIgk1mOroht03zw3+evaDOaccRVs2nWIzu5I2KGcQIleRDJSJOL8dFUjl51Zw4Sq1D0JO5Bzx5fT1eNs2nUo7FBOoEQvIhlpxZa9NO0/ysfrwu/NQ7RHD6TlzBslehHJSI/WN1JelMeHzhkTdigATB5ZwojCPNal4Q1ZJXoRyTgH27v41bqdLLhgPEX5yV+OOIicHGPmmDI27tTQjYjIaVu2toXO7khoT8IOZNroMjbtPpR2Wwsq0YtIxnlsVRMzx5Rx7vjysEM5zvTRIzjQ3kXr4WNhh3IcJXoRyShb9xxhTeMBbrxwPLFNjdLG9NFlALy963DIkRwvUKI3s/lmttHMGszs3n7OLzCztWa2xszqzezyoHVFRIbiidXNmMGCC8aHHcoJpo0eAZB2UywHTfRmlgs8AFwLzAJuNbNZfYr9Bpjt7hcAnwEeGkJdEZFA3J0n1jTznjOrGVNRFHY4J6gdUUhlST6bMrBHPw9ocPct7t4JPAIsiC/g7of9D3cfSgEPWldEJKjXth9g2952bkjD3jxEl0KYPqqMtzOtRw+MBxrj3jfFjh3HzG40s7eAJ4n26gPXFREJ4onVzRTm5TD/3PSYO9+faaNHsGlXes28CZLo+7vbcUIL3H2pu88EbgC+NpS6AGa2KDa+X9/a2hogLBEZTjq7I/xibQvXnDOGsqL8sMMZ0PTRZbR1dLP7UPrMvAmS6JuA+MmqE4CWgQq7+/PAmWZWM5S67r7E3evcva62NrV7PopI+vvdplYOtHdx44Xjwg7lpKaNit6QbdidPuP0QRL9SmCamU01swJgIbAsvoCZnWWxeU5mNgcoAPYGqSsiEsQTq5upLi3gimnp3RGcWlsKRKeBpou8wQq4e7eZ3QM8DeQCD7v7ejO7O3Z+MXATcIeZdQFHgVtiN2f7rZuktohIlmrr6OLZN3dx27xJ5Oem9+M/o8uKKMrPyaxED+Duy4HlfY4tjnt9P3B/0LoiIkPxqzd20Nkd4YYL038uR06OMaW6lHfSKNGn9/8aRUSApaubmVpTyuwJFWGHEsjUmtK06tEr0YtIWms+cJQVW/al5ZIHA5laU8r2fe1096THblNK9CKS1n6+JrpdYLo+JNWfqTWldEecpv1Hww4FUKIXkTTm7ix9rZm5k6uYVB3+doFBTa1Jr5k3SvQikrY27Gjj7d2HM+ImbLzeRL9FiV5E5OSWvtZMfq7xkfPGhh3KkIwsLaCsKC9tZt4o0YtIWuqJOD9/vYUrZ4yiqrQg7HCGxMzSauaNEr2IpKWXNu+h9dAxbsywYZteE0eW0Li/PewwACV6EUlTS19rpqwoj/fPHBV2KKdk0sgSWg4cpScS/iqWSvQiknbaO7t5av1OPnzeWIryc8MO55RMrCqhq8fZ2dYRdihK9CKSfp7dsIv2zp6Mm20Tb+LIYgAa94U/fKNELyJpZ+nqZsZXFjNvysiwQzllE6ui8/63K9GLiByv9dAxXnh7Dx+9YBw5OZmx5EF/xlUWYwZNSvQiIsf75doWeiKesbNtehXk5TCuopjGNFgGQYleRNLKE6ubmTW2nOmjy8IO5bRNqCrWGL2ISLzNrYd5velgxvfme6XLXHolehFJGz9f3YwZfPSC9N4XNqiJVSXsajtGR1dPqHEESvRmNt/MNppZg5nd28/5T5rZ2tjXS2Y2O+7cO2b2hpmtMbP6RAYvItnD3XliTQuXnVnD6PKisMNJiEnV0SmWYS9XPGiiN7Nc4AHgWmAWcKuZzepTbCvwPnc/H/gasKTP+avc/QJ3r0tAzCKShV7bfoDt+9ozeu58XxNiUyybD6R5ogfmAQ3uvsXdO4FHgAXxBdz9JXffH3u7ApiQ2DBFJNs9sbqZwrwcPnTO6LBDSZixFdG/THZkQKIfDzTGvW+KHRvIXcCv4t478IyZrTKzRQNVMrNFZlZvZvWtra0BwhKRbNHZHeGXa1u4etZoyoryww4nYUaXF2EGLSEn+rwAZfp7YqHfVXrM7Cqiif7yuMOXuXuLmY0CnjWzt9z9+RM+0H0JsSGfurq68FcBEpGUeX5TK/vbu7Jmtk2v/NwcRpUV0nIw3PVugvTom4CJce8nAC19C5nZ+cBDwAJ339t73N1bYt93A0uJDgWJiLxr6Zpmqkryee/02rBDSbixFcXsOJj+QzcrgWlmNtXMCoCFwLL4AmY2CXgcuN3dN8UdLzWzst7XwDXAukQFLyKZ71BHF7/esIvrZ48jPzf7ZnyPryxmx4Fwe/SDDt24e7eZ3QM8DeQCD7v7ejO7O3Z+MfBVoBr4jpkBdMdm2IwGlsaO5QE/cvenktISEclIT63bybHuSFbNtok3tqKI37y1C3cnlgtTLsgYPe6+HFje59jiuNefBT7bT70twOy+x0VEej2xppnJ1SVcOLEy7FCSYmxlMR1dEQ60d4W2JWL2/Z0kIhlj58EOXtq8lwUXjA+tt5ts42JTLFtCHKdXoheR0Cx7vRl3uCFLljzoz9jK6NOxYY7TK9GLSGiWrm5h9sRKzqgdEXYoSdPbow9z5o0SvYiEYuPOQ7y5o40bs7g3D1AzopD8XKNZPXoRGW6Wrm4mN8f4yOzsTvQ5Ocbo8iL16EVkeOmJOEtXN3Hl9FpqRhSGHU7SjasIdy69Er2IpNyLDXvY1XaMm+YOj/UPx1YWadaNiAwvj61qoqI4nw+cPSrsUFJiXGUxu9o6iETCWcZLiV5EUqqto4un1+/ko7PHUZiXG3Y4KTG2ooiuHmfPkWOhXF+JXkRS6sm1OzjWHRk2wzYAo8qiUyx3tynRi8gw8LNVTZw1agSzJ1SEHUrKjCqP3nDefSicG7JK9CKSMlv3HGHVtv3cNGdC1i550J/ePXDVoxeRrPfYqiZyjKzbYGQwtbEppLuU6EUkm0UiztLVzVw+rZYxsWUBhouCvBxGlhZo6EZEstuKLXtpPnCUm4fRTdh4o8oK1aMXkez2s1VNlBXlcc2s0WGHEopR5UW0pnOP3szmm9lGM2sws3v7Of9JM1sb+3rJzGYHrSsi2e/wsW5+tW4nHzl/HEX5w2PufF9p3aM3s1zgAeBaYBZwq5nN6lNsK/A+dz8f+BqwZAh1RSTLLV+7g6NdPdw8d3jdhI03uryQ1sPHQnk6NkiPfh7Q4O5b3L0TeARYEF/A3V9y9/2xtyuACUHrikj2e7S+kTNrS5kzqSrsUEIzqqyInoiz90hnyq8dJNGPBxrj3jfFjg3kLuBXp1hXRLLMpl2HWLVtPwsvmjSs5s73NTrEh6aCJPr+fjL9/u1hZlcRTfRfOYW6i8ys3szqW1tbA4QlIpng0ZWN5OcaN84Z3n282hCXQQiS6JuAiXHvJwAtfQuZ2fnAQ8ACd987lLoA7r7E3evcva62tjZI7CKS5o519/D4a01cPWv0sFh3/mTSvUe/EphmZlPNrABYCCyLL2Bmk4DHgdvdfdNQ6opI9np2wy72t3ex8KJJYYcSutqy8J6OzRusgLt3m9k9wNNALvCwu683s7tj5xcDXwWqge/ExuC6Y73zfusmqS0ikmYeebWR8ZXFXH5WTdihhK4wL5eqkvxQevSDJnoAd18OLO9zbHHc688Cnw1aV0SyX+O+dl5s2MOXPjidnJzhexM23qiyorQdoxcRGbKf1DeSY/DxuuG55EF/RpUXsuuQEr2IZIHungg/qW/kfdNrGVdZHHY4aSPao0/Pm7EiIkPyu02t7Go7xi26CXuc2rJC9h7uxD21T8cq0YtIwv341e3UjCgYNpt/B1UzooDOnghtHd0pva4SvYgkVNP+dn771m5uuWgi+blKMfGqRxQAsPdwasfp9VMQkYT68avbAbh1noZt+up9aGzP4dSud6NELyIJc6y7h0dXNvL+maOZUFUSdjhpp7o0mujVoxeRjPXUup3sOdzJ7ZdODjuUtFRTFh262aNELyKZ6ocrtjG5uoQr9CRsv0aWFGCmoRsRyVBv7mhj5Tv7+dTFk/Uk7ADycnOoKilg7xH16EUkA/1wxTYK83KG7ebfQVWXFrDnkHr0IpJhDnV0sXR1M9fPHkdVaUHY4aS1mhGF6tGLSOZZurqZ9s4ebr9EN2EHUz2iQGP0IpJZIhHney+9w+wJFcyeWBl2OGmvZkShZt2ISGb53aZWtrQe4TOXTw07lIxQM6KAQx3ddHT1pOyaSvQiclq+++JWRpcXct15Y8MOJSNUx56O3XckdcM3SvQicso27jzEiw17uOPSKVrXJqA/LIOQuuGbQD8ZM5tvZhvNrMHM7u3n/Ewze9nMjpnZl/uce8fM3jCzNWZWn6jARSR8D7+4laL8HD55sda1CeoPC5ulrkc/6FaCZpYLPABcDTQBK81smbtviCu2D/gCcMMAH3OVu+85zVhFJI3sOXyMpWua+fjcCVSWaEplULWxHn1rmvXo5wEN7r7F3TuBR4AF8QXcfbe7rwS6khCjiKShH72ync7uCHdeppuwQxFGjz5Ioh8PNMa9b4odC8qBZ8xslZktGqiQmS0ys3ozq29tbR3Cx4tIqh3r7uH7L2/jyhm1nDVqRNjhZJSSgjxKCnLTboy+v0UrhrIP1mXuPge4Fvi8mb23v0LuvsTd69y9rra2dggfLyKp9vM1Lew5fIy7NKXylFSPKEjpUsVBEn0TMDHu/QSgJegF3L0l9n03sJToUJCIZKieiLP4d5s5Z1w5l2uVylNSXVqY0qdjgyT6lcA0M5tqZgXAQmBZkA83s1IzK+t9DVwDrDvVYEUkfM9u2MmW1iP81yvPxEyrVJ6K6tIC9ren0awbd+82s3uAp4Fc4GF3X29md8fOLzazMUA9UA5EzOyLwCygBlga+2XIA37k7k8lpSUiknTuzoPPbWZKdQnXnqsHpE5VVWkBb+5oS9n1Bk30AO6+HFje59jiuNc7iQ7p9NUGzD6dAEUkfby0eS+vNx3kbz92Hrlac/6UjSwtYF8Ke/R6lE1EAnvwuc2MKivkY3OGMvFO+qosyaejK8LRztSsd6NELyKBrG06wIsNe7jr8qkU5uWGHU5GGxl7wCxVvXolehEJ5MHnNlNelMdtWu7gtPVuzrI/RQubKdGLyKA27jzEU+t3cselUygryg87nIw3MpboU7WCpRK9iAzq27/ZRGlBHp+9Qg9IJUJVbOgmVVMslehF5KTe3NHG8jd2cudlU7R4WYKM1NCNiKSTb//6bcoK8/js5WeEHUrWqCjOxwz2tadmHUglehEZ0PqWgzy1fid3Xj6VihKNzSdKbo5RWZyvHr2IhO/bv36bsqI87tJSxAlXVZK6h6aU6EWkX+uaD/LMhl185jL15pOhqrRAPXoRCdf9T71FZUk+n9FSxElRVVKg6ZUiEp4X3m7lhbf3cM9VZ1FRrN58MowszeeAbsaKSBgiEef+p95ifGUxt186OexwslZVbGEz96Hs43RqlOhF5Di/WNvCuuY2/vSa6VrTJolGlhTQ2R2hPQULmynRi8i7OrsjfP2ZjZw9tpwbLtAKlclUlcJlEJToReRd//HKNhr3HeUr82eQo/Xmk2pkCpdBCJTozWy+mW00swYzu7ef8zPN7GUzO2ZmXx5KXRFJD/uPdPKtX7/NZWdV877ptWGHk/XSqkdvZrnAA8C1RLcHvNXMZvUptg/4AvD1U6grImngH5/dyOFj3Xz1I+doL9gUqIo9m5AuPfp5QIO7b3H3TuARYEF8AXff7e4rgb5zhQatKyLh29DSxo9e2c7tl0xmxpiysMMZFv6wVHHyp1gGSfTjgca4902xY0GcTl0RSQF35y9+sZ6K4ny+9MHpYYczbJQX5ZNjcCBNevT9/Q0XdOJn4LpmtsjM6s2svrW1NeDHi8jpevKNHby6dR9f/tAMLXWQQjk5RmWKno4NkuibgIlx7ycALQE/P3Bdd1/i7nXuXldbqxtBIqlw5Fg3f/Pkm8waW87Ci7RFYKpVFudz8Gh6DN2sBKaZ2VQzKwAWAssCfv7p1BWRJPvGs5toOdjBXy44h1xNp0y5ipLUJPq8wQq4e7eZ3QM8DeQCD7v7ejO7O3Z+sZmNAeqBciBiZl8EZrl7W391k9QWERmCtU0H+Pffb+VTl0yibsrIsMMZliqK81MydDNoogdw9+XA8j7HFse93kl0WCZQXREJV1dPhHsfe4OaEYX82fyZYYczbFUW57Ol9UjSrxMo0YtIdnn4xa1s2NHG4k/NobxIN2DDUllSkDazbkQki2zf2843f72Jq2eN5kPnjAk7nGGtvDifto5ueiLJXcFSiV5kGOmJOF/+6evk5+Twlwv0BGzYKmNr/R/qSO4NWSV6kWHk4Re38uo7+7jvo+cwtqI47HCGvcrYcwvJ3oBEiV5kmNi06xD/8PRGrpk1mpvm6AH1dNC7e9eBJE+xVKIXGQY6uyN86dE1lBXl8TcfO09DNmmit0ef7Ln0mnUjMgx889ebWN/Sxr/ePpeaEYVhhyMxFcXRhc2SPfNGPXqRLPfcxt08+Nxmbp03SbNs0kyqevRK9CJZbOfBDv77T15n5pgy7rteW0Gkm94x+oO6GSsip6K7J8IXHllNR1cP/3LbHIrytdF3usnPzaG0IDfpN2M1Ri+Spb7+zCZe3bqPb3xiNmeNGhF2ODKA6NOx6tGLyBD9fE0zi38XHZf/2Jx+l6GSNFGegqWKlehFssy65oN85bG1XDSliv/70XPCDkcGEV2TXrNuRCSgPYeP8cc/WEVVSQHf+eRcCvL0n3i6qyzJT/rQjcboRbLE0c4e/uj79ew5fIyf3f0eass0Xz4TVBTn68lYERlcT8T5wiOrWdN4gG8vvJDzJlSEHZIE1LvLlHvyVrBUohfJcO7OfcvW8eyGXfzF9ecw/1w9FJVJKosL6OyO0NEVSdo1AiV6M5tvZhvNrMHM7u3nvJnZP8XOrzWzOXHn3jGzN8xsjZnVJzJ4EYF/+W0DP1yxnT9+7xl8+j1Twg5HhujdFSyTeEN20ERvZrnAA8C1wCzgVjPr+4jdtcC02Nci4ME+569y9wvcve70QxaRXkue38w/PruJj104nq9oS8CM9O7TsUkcpw/So58HNLj7FnfvBB4BFvQpswD4vketACrNbGyCYxWROP/vpXf4m+Vv8eHzx/L3N59PTo5WpMxEvZuPJHPmTZBEPx5ojHvfFDsWtIwDz5jZKjNbNNBFzGyRmdWbWX1ra2uAsESGrx+u2MZ9y9ZzzazRfOuWC8jL1e22TFWRgs1Hgvx29NdN6Ht7+GRlLnP3OUSHdz5vZu/t7yLuvsTd69y9rra2NkBYIsPTg89t5n8/sY4PzBzFP992IflK8hntD0M3IY7RE+2dT4x7PwFoCVrG3Xu/7waWEh0KEpEhcnfuf+ot7n/qLa6fPY7Ft8+lME8LlWW6inf3je1O2jWCJPqVwDQzm2pmBcBCYFmfMsuAO2Kzby4BDrr7DjMrNbMyADMrBa4B1iUwfpFhoasnwp8//gYPPreZ2y6exLduuUA9+SxRWpCHGbQl8WbsoE/Gunu3md0DPA3kAg+7+3ozuzt2fjGwHLgOaADagTtj1UcDS2PbluUBP3L3pxLeCpEsdrC9i8/9aBW/b9jL5686ky9fM0NbAWaRnByjrDCPtiT26AMtgeDuy4km8/hji+NeO/D5fuptAWafZowiw9a2vUe483sradzXztc/Ppub52olymxUXpwfbo9eRMLxn2/t5ks/WQPAD++6mIvPqA43IEma8qJ82jqU6EWGje6eCP/47CYefG4zZ48t58FPzmFKTWnYYUkSlRfn0XY05KEbEUmNlgNH+eIja3j1nX3cOm8S910/S1sADgPlRfls39eetM9XohdJA+7OT+ub+NovN9Djzjdvmc2NF2o8frjQGL1Iltt5sIM/f3wt/7mxlYunjuQfbp7NpOqSsMOSFIqO0WvoRiTrdPVE+Pffb+Xbv36bHnf+4vpZ3HHpFK1ZMwyVF+dx+Fg33T2RpCxnoUQvEoLfN+zhvmXradh9mPfPHMV9189icrVuuA5X5UXRp2MPH+umsqQg4Z+vRC+SQmubDvAPT2/khbf3MGlkCd/9dB0fOHt02GFJyMpjyyC0HVWiF8lYG3ce4pvPbuKp9TupKsnnf113NrdfOlkzagSA8qJoKk7WXHolepEkcXde3rKXJc9v4bmNrYwozOOLH5zGXZdPpSz2p7oIxPfolehFMsKRY908uXYHP1ixjTeaD1IzooA/vXo6n7pkMlWlif+zXDJf7xi9evQiaczdeb3pII+u3M6yNS0c6ezhrFEj+NuPnceNF47XEI2cVHlxbOgmSU/HKtGLnCJ3Z11zG8vX7WD5GzvYtredovwcPnL+OBZeNJG5k6u0yqQE8u7QjXr0IuFr7+zmla37eH5TK795czfb97WTm2O858xq7n7fmXz4/LHv/hkuEtSIJK9Jr0QvchLtnd283niQ17bv56XNe1i5dT+dPREK83K45IxqPn/VmVwza4zG3uW05OQYI5K4Jr0SvUhMR1cPDbsP89bOQ6xtOsBr2/fz5o5D9ESi2x/PGF3Gp98zmfdOr+WiKSM17i4JVV6UvPVuAiV6M5sPfJvoDlMPufvf9TlvsfPXEd1h6r+4+2tB6oqkkruz53An2/cdYdvedrbtbadh92He3NnGO3uOEMvplBTkcsHESj535ZnMmVTFhZMqk/Igi0iv8uLkrUk/aKI3s1zgAeBqopuArzSzZe6+Ia7YtcC02NfFwIPAxQHripw2d+doVw/727vY3dbB7kPH2H3oGK1xr1sOHGX7vnbaO3verWcGE6tKmDGmjI+cN5YZY8qZMaaMKdUlSVlzRGQg5UXJW5M+SI9+HtAQ2xYQM3sEWADEJ+sFwPdjWwquMLNKMxsLTAlQV7JUJOJ0R5zuSITuiNPT43RFIvREnO6e6LmeSISuHqerJ0JHV4SjXT10xL6OdvZwtCv61RF73d7ZQ1tHNwePdnHwaBeHYt/bOrro6vETYjCDmhGFjCorZEJVMe85s4ZJI4uZXF3KpOoSxlcWawhG0kJ5cT6NSVqTPkiiHw80xr1vItprH6zM+IB1E+Yj//wCHV0RINrD63XCf/7e78vj6px4Lv54n3Le/+sTLjtATH3rxH++DxDrifX6r3PitQb+dzm+HQP/Wwz07wfQE/FoIo9E3h0GSZTi/FyKC3IpL8qjojif8uJ8JlQVU1Gcf9zXqLJCRpUVMaq8kOrSAvXMJSOEPUbf30Tgvv8JD1QmSN3oB5gtAhYBTJo0KUBYJzqrdsTxvTrr92Xv9fo913fa8/Hn+q9z4rXiyp308/qvc8K5406dpNwAx08W08n+XfoaKN744zkGebk55OUYuTlGfm4OuTlGXuwrNzeH/P7O5ebEEnkORfm5FOfnvvu9uCCXwrwczUmXrDZ3chX5ucn5HQ+S6JuAiXHvJwAtAcsUBKgLgLsvAZYA1NXVnVJf8FsLLzyVaiIiobvt4kncdvGpdXIHE+Rv2pXANDObamYFwEJgWZ8yy4A7LOoS4KC77whYV0REkmjQHr27d5vZPcDTRKdIPuzu683s7tj5xcByolMrG4hOr7zzZHWT0hIREemX9b3plg7q6uq8vr4+7DBERDKGma1y97r+zmk6gohIllOiFxHJckr0IiJZToleRCTLKdGLiGS5tJx1Y2atwLZTrF4D7ElgOJlAbc5+w629oDYP1WR3r+3vRFom+tNhZvUDTTHKVmpz9htu7QW1OZE0dCMikuWU6EVEslw2JvolYQcQArU5+w239oLanDBZN0YvIiLHy8YevYiIxMnIRG9m881so5k1mNm9/Zw3M/un2Pm1ZjYnjDgTKUCbPxlr61oze8nMZocRZyIN1ua4cheZWY+Z3ZzK+JIhSJvN7EozW2Nm683sd6mOMdEC/G5XmNkvzOz1WJvvDCPORDGzh81st5mtG+B84vOXu2fUF9HljjcDZxDd2OR1YFafMtcBvyK6gdIlwCthx52CNr8HqIq9vnY4tDmu3G+JLpV9c9hxp+DnXEl0z+VJsfejwo47BW3+n8D9sde1wD6gIOzYT6PN7wXmAOsGOJ/w/JWJPfp3Nyt3906gd8PxeO9uVu7uK4Dezcoz1aBtdveX3H1/7O0Kort5ZbIgP2eAPwEeA3anMrgkCdLm24DH3X07gLtneruDtNmBMovuJTmCaKLvTm2YiePuzxNtw0ASnr8yMdEPtBH5UMtkkqG25y6iPYJMNmibzWw8cCOwOIVxJVOQn/N0oMrMnjOzVWZ2R8qiS44gbf4X4Gyi25C+Afw3d4+kJrxQJDx/BdkzNt2czmblmWoom6xfRTTRX57UiJIvSJu/BXzF3XuyZOPwIG3OA+YCHwCKgZfNbIW7b0p2cEkSpM0fAtYA7wfOBJ41sxfcvS3JsYUl4fkrExP96WxWnqkCtcfMzgceAq51970pii1ZgrS5DngkluRrgOvMrNvdn0hJhIkX9Hd7j7sfAY6Y2fPAbCBTE32QNt8J/J1HB7AbzGwrMBN4NTUhplzC81cmDt2czmblmWrQNpvZJOBx4PYM7t3FG7TN7j7V3ae4+xTgZ8DnMjjJQ7Df7Z8DV5hZnpmVABcDb6Y4zkQK0ubtRP+CwcxGAzOALSmNMrUSnr8yrkfvp7FZeaYK2OavAtXAd2I93G7P4AWhArY5qwRps7u/aWZPAWuBCPCQu/c7TS8TBPw5fw34npm9QXRY4yvunrGrWprZj4ErgRozawLuA/IheflLT8aKiGS5TBy6ERGRIVCiFxHJckr0IiJZToleRCTLKdGLiGQ5JXoRkSynRC8ikuWU6EVEstz/B0MXRrpU+R0OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use np.linspace to get a NumPy array of evenly spaced values for r.\n",
    "r_vals = np.linspace(0, 1, 1000)\n",
    "probabilities = np.zeros_like(r_vals)\n",
    "\n",
    "N = 10\n",
    "y = 9\n",
    "\n",
    "for idx, r in enumerate(r_vals):\n",
    "    current_prob = binom.pmf(y, N, r)\n",
    "    probabilities[idx] = current_prob\n",
    "\n",
    "plt.plot(r_vals, probabilities)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "\n",
    "We will define three different \"predictor\" classes. The first will use maximum likelihood to make its predictions, the other two will use a Bayesian approach, with different kinds of priors. Each predictor object will have the following functions:\n",
    "\n",
    "* `predictor.update(num_heads, N)` returns a new predictor object. This predictor object takes into account that `N` new data points have been seen, `num_heads` of which were heads.\n",
    "\n",
    "* `predictor.predict()` returns a number between 0 and 1, which is the probability that this predictor assigns to the next coin flip coming up heads.\n",
    "\n",
    "* `predictor.marginal_likelihood(num_heads, N)` returns the Bayesian marginal likelihood. This is only defined for Bayesian predictors; for other types, it returns `np.nan` (\"not a number\").\n",
    "\n",
    "The definition of the `MaximumLikelihoodPredictor` predictor class is given below (you don't need to change it), with some testing code to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: MaxiLikeliPredctr with num_heads = 0, N = 0\n",
      "After: MaxiLikeliPredctr with num_heads = 2, N = 2\n",
      "prediction = 1.0\n"
     ]
    }
   ],
   "source": [
    "class MaximumLikelihoodPredictor:\n",
    "    def __init__(self):\n",
    "        self.num_heads = 0\n",
    "        self.N = 0\n",
    "    def __str__(self):\n",
    "        return f\"MaxiLikeliPredctr with num_heads = {self.num_heads}, N = {self.N}\"\n",
    "    def update(self, num_heads, N):\n",
    "        after = MaximumLikelihoodPredictor()\n",
    "        after.num_heads += num_heads\n",
    "        after.N += N\n",
    "        return after\n",
    "    def predict(self):\n",
    "        return self.num_heads / self.N\n",
    "    def get_marginal_likelihood(self, num_heads, N):\n",
    "        return np.nan\n",
    "\n",
    "# ██████████ TEST ██████████\n",
    "predictor = MaximumLikelihoodPredictor()\n",
    "print(\"Before:\", predictor)\n",
    "# After two flips, both heads, you should get:\n",
    "# After: MaximumLikelihoodPredictor with num_heads = 2, N = 2\n",
    "# which should predict that the next flip will land heads with probability 1.0\n",
    "predictor_after = predictor.update(2, 2)\n",
    "print(\"After:\", predictor_after)\n",
    "print(\"prediction =\", predictor_after.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two Bayesian predictor classes will be defined next. The first will use a discrete prior: instead of considering the entire range from 0 to 1 (containing infinitely many numbers) as possible values for $R$, it will only consider some finite set of such numbers. It assigns some probability to each of them, and these probabilities add up to 1. This finite set and the associated probabilities can be represented in Python by a dictionary, that maps a value $r$ to the probability assigned to it by the prior. For example, `prior.probability_for_r[.5] = .2` means that $P(R = .5) = .2$.\n",
    "\n",
    "Being a Bayesian predictor, the `update` function will use Bayes' theorem to compute the posterior distribution after having seen the data. For a discrete prior, the posterior will assign probability to the same values $r$, but the probability assigned may be different.\n",
    "\n",
    "**Task 2** (1 point): Complete the implementation of the `update` function below. (You'll finish to other incomplete functions in later tasks.) You can use the provided testing code to see if the posterior looks as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: DiscrtPriorPredctr with P(R = 0.3333) = 0.5000; P(R = 0.6667) = 0.5000\n",
      "Posterior: DiscrtPriorPredctr with P(R = 0.3333) = 0.2000; P(R = 0.6667) = 0.8000\n",
      "prediction = 0.6\n",
      "marginal likelihood = 0.27777777777777773\n"
     ]
    }
   ],
   "source": [
    "class DiscretePriorPredictor:\n",
    "    def __init__(self):\n",
    "        self.probability_for_r = {}\n",
    "    def __str__(self):\n",
    "        return (\"DiscrtPriorPredctr with \"\n",
    "                + \"; \".join([f\"P(R = {r:.4f}) = {prior_prob:.4f}\"\n",
    "                             for r, prior_prob in self.probability_for_r.items()]))\n",
    "    def update(self, num_heads, N):\n",
    "        if not len(self.probability_for_r):\n",
    "            raise ValueError(\"assign probabilities for DiscretePriorPredictor before calling update\")\n",
    "\n",
    "        posterior = DiscretePriorPredictor()\n",
    "        total_prob = 0.0\n",
    "        for r, prior_prob in self.probability_for_r.items():\n",
    "            # Compute the value in the numerator of Bayes' theorem, and\n",
    "            # assign it to posterior.probability_for_r[r]\n",
    "            numerator = binom.pmf(num_heads, N, r) * self.probability_for_r[r]\n",
    "            posterior.probability_for_r[r] = numerator\n",
    "            total_prob += numerator\n",
    "\n",
    "        # Finaly, divide everything by the sum\n",
    "        for r, posterior_prob in posterior.probability_for_r.items():\n",
    "            posterior.probability_for_r[r] = np.float64(posterior.probability_for_r[r]) / np.float64(total_prob)\n",
    "        return posterior\n",
    "    def predict(self):\n",
    "        if not len(self.probability_for_r):\n",
    "            raise ValueError(\"assign probabilities for DiscretePriorPredictor before calling predict\")\n",
    "\n",
    "        summed_prob = 0\n",
    "        for r, prob in self.probability_for_r.items():\n",
    "            summed_prob += r * prob\n",
    "        return summed_prob\n",
    "    def get_marginal_likelihood(self, num_heads, N):\n",
    "        if not len(self.probability_for_r):\n",
    "            raise ValueError(\"assign probabilities for DiscretePriorPredictor before calling get_marginal_likelihood\")\n",
    "\n",
    "        marginal_sum = 0\n",
    "        for r in self.probability_for_r.keys():\n",
    "            marginal_sum += binom.pmf(num_heads, N, r) * self.probability_for_r[r]\n",
    "        return marginal_sum\n",
    "\n",
    "# ██████████ TEST ██████████\n",
    "discrete_prior = DiscretePriorPredictor()\n",
    "discrete_prior.probability_for_r[1/3] = 1/2\n",
    "discrete_prior.probability_for_r[2/3] = 1/2\n",
    "print(\"Prior:\", discrete_prior)\n",
    "# After two flips, both heads, you should get:\n",
    "# Posterior: DiscretePrior with P(R = 0.3333) = 0.2000; P(R = 0.6667) = 0.8000,\n",
    "# which should predict that the next flip will land heads with probability 0.6.\n",
    "# The marginal likelihood for these data and prior should be ~ 0.2777.\n",
    "discrete_posterior = discrete_prior.update(2, 2)\n",
    "print(\"Posterior:\", discrete_posterior)\n",
    "print(\"prediction =\", discrete_posterior.predict())\n",
    "print(\"marginal likelihood =\", discrete_prior.get_marginal_likelihood(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian predictive distribution is the distribution over a new coin flip, conditioned on the data. Calling the `predict` function on the posterior should output the probability $P(\\text{new flip = heads} | \\text{previous data})$. You saw how to compute it on the final slide of lecture 5.\n",
    "\n",
    "**Task 3** (1 point): Implement the `predict` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the marginal likelihood is the quantity that occurs in the denominator of Bayes' Theorem. In this case, the prior is discrete, so the marginal likelihood is given by a sum.\n",
    "\n",
    "The marginal likelihood is *not conditioned on the data*. In our code, the `posterior` object (computed by `update`) can tell us things that involve conditioning on the data. But for the marginal likelihood, we'll need to origina `prior` object. You'll see that the testing code calls `prior.get_marginal_likelihood`, not on `posterior.get_marginal_likelihood`.\n",
    "\n",
    "**Task 4** (0.5 points): Implement the `get_marginal_likelihood` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** (1 point): Write a function that visualises a `DiscretePriorPredictor` using vertical lines whose height denotes the probability attached to some value of $r$: something like [this figure from Wikipedia](https://en.wikipedia.org/wiki/Discrete_uniform_distribution#/media/File:Uniform_discrete_pmf_svg.svg). Use `plt.vlines`. Set the horizontal axis to run from -0.05 to 1.05. Use your function to plot the prior and the posterior from the test code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.vlines(discrete_prior.probability_for_r.keys(),\n",
    "           [0 for _ in range(len(discrete_posterior.probability_for_r))],\n",
    "           discrete_prior.probability_for_r.values(),\n",
    "           color='green')\n",
    "plt.vlines(discrete_posterior.probability_for_r.keys(),\n",
    "           [0 for _ in range(len(discrete_posterior.probability_for_r))],\n",
    "           discrete_posterior.probability_for_r.values(),\n",
    "           color='blue', linestyles='dotted')\n",
    "\n",
    "plt.xlim(-0.05, 1.05);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final predictor class will use the Beta distribution. This is a continuous prior that considers all values of $r$ between 0 and 1. It is the conjugate prior for the binomial likelihood, which has some major computational advantages.\n",
    "\n",
    "**Task 6** (1 point): Implement `update` and `predict` below for the `BetaPriorPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: BetaPriorPredctr with alpha = 1.0000, beta = 1.0000\n",
      "Posterior: BetaPriorPredctr with alpha = 3.0000, beta = 1.0000\n",
      "prediction = 0.75\n",
      "marginal likelihood = 0.3333333333333331\n"
     ]
    }
   ],
   "source": [
    "class BetaPriorPredictor:\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    def __str__(self):\n",
    "        return f\"BetaPriorPredctr with alpha = {self.alpha:.4f}, beta = {self.beta:.4f}\"\n",
    "    def update(self, num_heads, N):\n",
    "        # We need alpha and beta for the beta dist.\n",
    "        a = self.alpha + num_heads\n",
    "        b = self.beta + (N - num_heads)\n",
    "        return BetaPriorPredictor(a, b)\n",
    "    def predict(self):\n",
    "        return (self.alpha / (self.alpha + self.beta)) # Expected value of beta dist\n",
    "    def get_marginal_likelihood(self, num_heads, N):\n",
    "        if self.alpha < 1e-9 or self.beta < 1e-9:\n",
    "            return np.nan\n",
    "        # The Gamma function may output really large numbers. To avoid\n",
    "        # numerical inaccuracy, the following code works with logarithms instead,\n",
    "        # converting back to the actual number when all Gamma's have been combined.\n",
    "        return (#math.comb(N, num_heads) # since version 3.8\n",
    "                sps.binom(N, num_heads)\n",
    "                * math.exp(math.lgamma(self.alpha + self.beta)\n",
    "                           - math.lgamma(self.alpha)\n",
    "                           - math.lgamma(self.beta)\n",
    "                           + math.lgamma(self.alpha + num_heads)\n",
    "                           + math.lgamma(self.beta + N - num_heads)\n",
    "                           - math.lgamma(self.alpha + self.beta + N)\n",
    "                          )\n",
    "               )\n",
    "    \n",
    "# ██████████ TEST ██████████\n",
    "beta_prior = BetaPriorPredictor(1, 1)\n",
    "print(\"Prior:\", beta_prior)\n",
    "# After two flips, both heads, you should get:\n",
    "# Posterior: BetaPrior with alpha = 3.0000, beta = 1.0000\n",
    "# which should predict that the next flip will land heads with probability 0.75\n",
    "# The marginal likelihood for these data and prior should be ~ 0.3333.\n",
    "beta_posterior = beta_prior.update(2, 2)\n",
    "print(\"Posterior:\", beta_posterior)\n",
    "print(\"prediction =\", beta_posterior.predict())\n",
    "print(\"marginal likelihood =\", beta_prior.get_marginal_likelihood(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7** (0.5 points): Write a function that visualises a `BetaPriorPredictor` by plotting its probability density function. You can use the `beta` object from `scipy.stats`, which has already been imported. Use your function to plot the prior and the posterior from the test code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting_r_7 = np.linspace(10**-4, 1-(10**-4))\n",
    "\n",
    "plt.plot(plotting_r_7,\n",
    "         beta_dist.pdf(plotting_r_7,\n",
    "                       beta_posterior.alpha,\n",
    "                       beta_posterior.beta),\n",
    "         color='green');\n",
    "plt.plot(plotting_r_7, \n",
    "         beta_dist.pdf(plotting_r_7,\n",
    "                       beta_prior.alpha,\n",
    "                       beta_prior.beta),\n",
    "         linestyle='dotted',\n",
    "         color='green');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** (0.5 points): For what values of alpha and beta does the Beta distribution have the same shape as the likelihood you plotted in task 1? Explain your answer, and verify it by showing the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The shape of the liklihood is the same for $\\alpha=10,\\beta=2$. That's because for those values, we 'update' the intial prior to look just like the prior we had in 1. (The current distribution has now become the same as immediately initalising it with those values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior: BetaPriorPredctr with alpha = 10.0000, beta = 2.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta_prior_8 = BetaPriorPredictor(1+9,1+1)\n",
    "print(\"Posterior:\", beta_prior_8)\n",
    "\n",
    "plotting_r_8 = np.linspace(10**-4, 1-(10**-4))\n",
    "plt.plot(plotting_r_8,\n",
    "         beta_dist.pdf(plotting_r_8,\n",
    "                       beta_prior_8.alpha,\n",
    "                       beta_prior_8.beta),\n",
    "         color='green');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with different predictors\n",
    "\n",
    "Next, we want to see the predictors in action. First, the code below creates a list of ten predictors, using the three classes defined above. We'll investigate their behaviour on several different values of the true probability of getting heads. These probabilities are defined below by `true_rs = [0, .3, .5, 2/3, .75, .999]`.\n",
    "\n",
    "Note that the final predictor in the list, `BetaPriorPredictor(0, 0)`, doesn't satisfy the constraints for being a proper Beta prior: that requires $\\alpha > 0$ and $\\beta > 0$. The problem is in the normalisation constant, which is undefined in this case. As we've seen, we can often get away with ignoring the normalisation constant, and by carrying this a bit further, we can use this unnormalised distribution as a prior in Bayes' theorem. Such a thing is called a *degenerate prior*. Often (but not always), the posterior we find will be a proper distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of predictors to be compared.\n",
    "predictors = [MaximumLikelihoodPredictor()]\n",
    "\n",
    "prior = DiscretePriorPredictor()\n",
    "prior.probability_for_r[1/3] = 1/2\n",
    "prior.probability_for_r[2/3] = 1/2\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = DiscretePriorPredictor()\n",
    "prior.probability_for_r[1/3] = 1/3\n",
    "prior.probability_for_r[1/2] = 1/3\n",
    "prior.probability_for_r[2/3] = 1/3\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = DiscretePriorPredictor()\n",
    "prior.probability_for_r[0] = 1/2\n",
    "prior.probability_for_r[1] = 1/2\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = DiscretePriorPredictor()\n",
    "for r in np.linspace(0, 1, 11):\n",
    "    prior.probability_for_r[r] = 1/11\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = DiscretePriorPredictor()\n",
    "for r in np.linspace(0, 1, 101):\n",
    "    prior.probability_for_r[r] = 1/101\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = BetaPriorPredictor(1, 1)\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = BetaPriorPredictor(3, 3)\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = BetaPriorPredictor(.5, .5)\n",
    "predictors.append(prior)\n",
    "\n",
    "prior = BetaPriorPredictor(0, 0)\n",
    "predictors.append(prior)\n",
    "\n",
    "true_rs = [0, .3, .5, 2/3, .75, .999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each combination of a predictor and a value $r_\\text{true}$ from `true_rs`, we will sample some training data (say 100 coin flips) and compute an updated predictor using these data. For the Bayesian predictors, this comes down to computing the posterior distribution. Then we'll sample some test data, say 100 points again. For both the training and the test data, the true probability of getting heads on each flip is given by $r_\\text{true}$.\n",
    "\n",
    "Now we want to evaluate how well the predictor does at predicting the test data. There are several ways to measure this. We'll try two different ones: the *logarithmic loss* and the *logarithmic regret*.\n",
    "\n",
    "The **logarithmic loss** is something you may have seen before in other places. For instance, logistic regression is training to minimise this loss, and it's also one of the most popular choices when training neural networks for binary classification problems (there you may have seen it under the name \"cross entropy\"). Suppose the predictor assigns probability $\\hat{r}$ to the outcome heads for a new coin flip $Y_\\text{new}$, and that a new flip actually comes out as $y_\\text{new}$ (1 for heads, 0 for tails). Then the logarithmic loss of this prediction is\n",
    "$$-\\log P_{\\hat{r}} ( Y_\\text{new} = y_\\text{new} ).$$\n",
    "In other words, if the coin in the test data came up heads, the loss is $-\\log (\\hat{r})$; if it came up tails, the loss is $-\\log (1 - \\hat{r})$.\n",
    "\n",
    "The logarithm of 1 is 0, and as $x$ goes down to $0$, $\\log(x)$ goes down to $-\\infty$. So we see that the logarithmic loss is a small positive number if the outcome of the test data point was something the predictor thought was probably going to happen. But if something happens that the predictor thought would happen only with a small probability, the logarithmic loss will be larger.\n",
    "\n",
    "Implementation note: because the logarithm of 0 is minus infinity, write your code in such a way that if no flip happened for which the predicted probability was 0, then $\\log(0)$ isn't called.\n",
    "\n",
    "One issue with logarithmic loss (or any loss, for that matter) is, that it's easier for predictors to get low average losses if $r_\\text{true}$ is close to 0 or 1, but much harder if it's closer to 0.5. To make the comparison more fair, we can look at the **regret** corresponding to the loss, so logarithmic regret in our case. The regret equals the loss of a predictor *minus* the loss of an ideal predictor that already knew the value $r_\\text{true}$. In other words, the ideal predictor uses $r_\\text{true}$ in the place where other predictors use $\\hat{r}$. By comparing our predictor's loss to the loss even a clairvoyant predictor would obtain, we'll get numbers that are more meaningful when different values of $r_\\text{true}$ are involved.\n",
    "\n",
    "**Task 9** (1.5 points): For each combination of predictor and $r_\\text{true}$, repeat this process 100 times (including getting new training and new test data every time), and store the following in three NumPy arrays:\n",
    "\n",
    "* The average loss;\n",
    "\n",
    "* The average regret;\n",
    "\n",
    "* The average marginal likelihood of the *training data* (don't use the test data for this one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DK\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "N_train = 100\n",
    "N_test = 100\n",
    "N_iter = 100\n",
    "\n",
    "losses = np.full((len(true_rs), len(predictors), N_iter), None)\n",
    "regrets = np.full((len(true_rs), len(predictors), N_iter), None)\n",
    "likelihoods = np.full((len(true_rs), len(predictors), N_iter), None)\n",
    "\n",
    "# For each true r,\n",
    "for r_idx, r_true in enumerate(true_rs):\n",
    "    for prior_idx, prior in enumerate(predictors): # Consider each prior\n",
    "        for N in range(N_iter): # Repeat the experiment N_iter times\n",
    "\n",
    "            # Generate the data\n",
    "            heads_train = np.random.binomial(N_train, r_true)\n",
    "            heads_test = np.random.binomial(N_test, r_true)\n",
    "            # Use it to update the prior\n",
    "            posterior = prior.update(heads_train, N_train) #N or N_train\n",
    "            # get r-hat after evidence was observed\n",
    "            r_hat = posterior.predict()\n",
    "\n",
    "            # use r_hat and r_true to calculate the loss with respect to the test-set\n",
    "            if   (r_hat == 1 and heads_test == N_test):   curr_loss = (heads_test) * -1 * np.log(r_hat)                  # r_hat = 1 & only heads\n",
    "            elif (r_hat == 1 and heads_test != N_test):   curr_loss = np.inf                                             # r_hat = 1 & not only heads\n",
    "            elif (r_hat == 0 and heads_test == 0):        curr_loss = (N_test - heads_test) * -1 * np.log(1 - r_hat)     # r_hat = 0 & only tails\n",
    "            elif (r_hat == 0 and heads_test != 0):        curr_loss = np.inf                                             # r_hat = 0 & not only tails\n",
    "            else: curr_loss = (heads_test) * -1 * np.log(r_hat) + (N_test - heads_test) * -1 * np.log(1 - r_hat)\n",
    "\n",
    "            # de regret is nu loss_rhat - loss_rtrue\n",
    "            if   (r_true == 1 and heads_test == N_test):  curr_true_loss = (heads_test) * -1 * np.log(r_true)\n",
    "            elif (r_true == 1 and heads_test != N_test):  curr_true_loss = np.inf\n",
    "            elif (r_true == 0 and heads_test == 0):       curr_true_loss = (N_test - heads_test) * -1 * np.log(1 - r_true)\n",
    "            elif (r_true == 0 and heads_test != 0):       curr_true_loss = np.inf\n",
    "            else: curr_true_loss = (heads_test) * -1 * np.log(r_true) + (N_test - heads_test) * -1 * np.log(1 - r_true)\n",
    "\n",
    "            # calculate the regret loss\n",
    "            curr_regret = curr_loss - curr_true_loss\n",
    "            # calculate the marginal likelihood\n",
    "            curr_likelihood = prior.get_marginal_likelihood(heads_train, N_train)\n",
    "\n",
    "            # Save the losses, regrets and marginal liklihoods (in order to calculate their mean)\n",
    "            losses[r_idx, prior_idx, N] = curr_loss\n",
    "            regrets[r_idx, prior_idx, N] = curr_regret\n",
    "            likelihoods[r_idx, prior_idx, N] = curr_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStr(FloatNumber, Precision):\n",
    "    return \"%0.*f\" % (Precision, FloatNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.300</th>\n",
       "      <th>0.500</th>\n",
       "      <th>0.667</th>\n",
       "      <th>0.750</th>\n",
       "      <th>0.999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaxiLikeliPredctr</th>\n",
       "      <td>-0</td>\n",
       "      <td>61.619</td>\n",
       "      <td>69.6941</td>\n",
       "      <td>64.3595</td>\n",
       "      <td>56.6721</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>40.5465</td>\n",
       "      <td>61.2993</td>\n",
       "      <td>74.064</td>\n",
       "      <td>63.864</td>\n",
       "      <td>57.6118</td>\n",
       "      <td>40.6366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>40.5465</td>\n",
       "      <td>61.7074</td>\n",
       "      <td>69.7335</td>\n",
       "      <td>63.9137</td>\n",
       "      <td>57.7525</td>\n",
       "      <td>40.6228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.000265611</td>\n",
       "      <td>61.6009</td>\n",
       "      <td>69.8104</td>\n",
       "      <td>64.7561</td>\n",
       "      <td>57.3301</td>\n",
       "      <td>2.46401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.568574</td>\n",
       "      <td>61.1149</td>\n",
       "      <td>70.0465</td>\n",
       "      <td>64.3485</td>\n",
       "      <td>56.731</td>\n",
       "      <td>1.28086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.98523</td>\n",
       "      <td>61.6534</td>\n",
       "      <td>70.0097</td>\n",
       "      <td>64.4312</td>\n",
       "      <td>56.3906</td>\n",
       "      <td>1.4989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>2.87101</td>\n",
       "      <td>61.9979</td>\n",
       "      <td>69.7018</td>\n",
       "      <td>64.3545</td>\n",
       "      <td>56.5267</td>\n",
       "      <td>3.36676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.496279</td>\n",
       "      <td>61.7074</td>\n",
       "      <td>69.8535</td>\n",
       "      <td>63.8822</td>\n",
       "      <td>56.6175</td>\n",
       "      <td>1.14661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>-0</td>\n",
       "      <td>61.3535</td>\n",
       "      <td>69.7362</td>\n",
       "      <td>64.4125</td>\n",
       "      <td>56.7035</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0.000    0.300    0.500    0.667    0.750    0.999\n",
       "MaxiLikeliPredctr            -0   61.619  69.6941  64.3595  56.6721      inf\n",
       "DiscrtPriorPredctr      40.5465  61.2993   74.064   63.864  57.6118  40.6366\n",
       "DiscrtPriorPredctr      40.5465  61.7074  69.7335  63.9137  57.7525  40.6228\n",
       "DiscrtPriorPredctr           -0      NaN      NaN      NaN      NaN      NaN\n",
       "DiscrtPriorPredctr  0.000265611  61.6009  69.8104  64.7561  57.3301  2.46401\n",
       "DiscrtPriorPredctr     0.568574  61.1149  70.0465  64.3485   56.731  1.28086\n",
       "BetaPriorPredctr        0.98523  61.6534  70.0097  64.4312  56.3906   1.4989\n",
       "BetaPriorPredctr        2.87101  61.9979  69.7018  64.3545  56.5267  3.36676\n",
       "BetaPriorPredctr       0.496279  61.7074  69.8535  63.8822  56.6175  1.14661\n",
       "BetaPriorPredctr             -0  61.3535  69.7362  64.4125  56.7035      inf"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_df = pd.DataFrame(np.mean(losses, axis = 2),\n",
    "             index = [printStr(r, 3) for r in true_rs], \n",
    "             columns = [str(a).split(' ', 1)[0] for a in predictors])\n",
    "l_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.300</th>\n",
       "      <th>0.500</th>\n",
       "      <th>0.667</th>\n",
       "      <th>0.750</th>\n",
       "      <th>0.999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaxiLikeliPredctr</th>\n",
       "      <td>0</td>\n",
       "      <td>0.422444</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>0.835147</td>\n",
       "      <td>0.471519</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>40.5465</td>\n",
       "      <td>0.263745</td>\n",
       "      <td>4.74929</td>\n",
       "      <td>-5.0637e-07</td>\n",
       "      <td>1.79575</td>\n",
       "      <td>39.6387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>40.5465</td>\n",
       "      <td>0.197291</td>\n",
       "      <td>0.418751</td>\n",
       "      <td>0.153722</td>\n",
       "      <td>1.71673</td>\n",
       "      <td>39.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.000265611</td>\n",
       "      <td>0.548328</td>\n",
       "      <td>0.495721</td>\n",
       "      <td>0.483136</td>\n",
       "      <td>0.448404</td>\n",
       "      <td>1.60421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.568574</td>\n",
       "      <td>0.486023</td>\n",
       "      <td>0.731804</td>\n",
       "      <td>0.366694</td>\n",
       "      <td>0.47549</td>\n",
       "      <td>0.352001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.98523</td>\n",
       "      <td>0.482238</td>\n",
       "      <td>0.694948</td>\n",
       "      <td>0.594954</td>\n",
       "      <td>0.266946</td>\n",
       "      <td>0.777244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>2.87101</td>\n",
       "      <td>0.64885</td>\n",
       "      <td>0.387045</td>\n",
       "      <td>0.476616</td>\n",
       "      <td>0.315154</td>\n",
       "      <td>2.36883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.496279</td>\n",
       "      <td>0.299005</td>\n",
       "      <td>0.538782</td>\n",
       "      <td>0.517285</td>\n",
       "      <td>0.526767</td>\n",
       "      <td>0.355885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0</td>\n",
       "      <td>0.572053</td>\n",
       "      <td>0.421502</td>\n",
       "      <td>0.583171</td>\n",
       "      <td>0.788617</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0.000     0.300     0.500       0.667     0.750  \\\n",
       "MaxiLikeliPredctr             0  0.422444  0.379382    0.835147  0.471519   \n",
       "DiscrtPriorPredctr      40.5465  0.263745   4.74929 -5.0637e-07   1.79575   \n",
       "DiscrtPriorPredctr      40.5465  0.197291  0.418751    0.153722   1.71673   \n",
       "DiscrtPriorPredctr            0       NaN       NaN         NaN       NaN   \n",
       "DiscrtPriorPredctr  0.000265611  0.548328  0.495721    0.483136  0.448404   \n",
       "DiscrtPriorPredctr     0.568574  0.486023  0.731804    0.366694   0.47549   \n",
       "BetaPriorPredctr        0.98523  0.482238  0.694948    0.594954  0.266946   \n",
       "BetaPriorPredctr        2.87101   0.64885  0.387045    0.476616  0.315154   \n",
       "BetaPriorPredctr       0.496279  0.299005  0.538782    0.517285  0.526767   \n",
       "BetaPriorPredctr              0  0.572053  0.421502    0.583171  0.788617   \n",
       "\n",
       "                       0.999  \n",
       "MaxiLikeliPredctr        inf  \n",
       "DiscrtPriorPredctr   39.6387  \n",
       "DiscrtPriorPredctr    39.763  \n",
       "DiscrtPriorPredctr       NaN  \n",
       "DiscrtPriorPredctr   1.60421  \n",
       "DiscrtPriorPredctr  0.352001  \n",
       "BetaPriorPredctr    0.777244  \n",
       "BetaPriorPredctr     2.36883  \n",
       "BetaPriorPredctr    0.355885  \n",
       "BetaPriorPredctr         inf  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_df = pd.DataFrame(np.mean(regrets, axis = 2),\n",
    "             index = [printStr(r, 3) for r in true_rs], \n",
    "             columns = [str(a).split(' ', 1)[0] for a in predictors]).round(1)\n",
    "r_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.300</th>\n",
       "      <th>0.500</th>\n",
       "      <th>0.667</th>\n",
       "      <th>0.750</th>\n",
       "      <th>0.999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MaxiLikeliPredctr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>1.22983e-18</td>\n",
       "      <td>0.0280742</td>\n",
       "      <td>0.00325072</td>\n",
       "      <td>0.0303582</td>\n",
       "      <td>0.0142947</td>\n",
       "      <td>2.12576e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>8.19885e-19</td>\n",
       "      <td>0.0170564</td>\n",
       "      <td>0.0216698</td>\n",
       "      <td>0.0208027</td>\n",
       "      <td>0.00898477</td>\n",
       "      <td>3.63209e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.0909115</td>\n",
       "      <td>0.00903225</td>\n",
       "      <td>0.0090073</td>\n",
       "      <td>0.00903628</td>\n",
       "      <td>0.00896276</td>\n",
       "      <td>0.0800066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscrtPriorPredctr</th>\n",
       "      <td>0.0155655</td>\n",
       "      <td>0.00980296</td>\n",
       "      <td>0.00980296</td>\n",
       "      <td>0.00980296</td>\n",
       "      <td>0.00980296</td>\n",
       "      <td>0.0151145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.00990099</td>\n",
       "      <td>0.00990099</td>\n",
       "      <td>0.00990099</td>\n",
       "      <td>0.00990099</td>\n",
       "      <td>0.00990099</td>\n",
       "      <td>0.00990099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>5.33447e-05</td>\n",
       "      <td>0.0128259</td>\n",
       "      <td>0.0178762</td>\n",
       "      <td>0.0141058</td>\n",
       "      <td>0.0107185</td>\n",
       "      <td>5.74868e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>0.0563485</td>\n",
       "      <td>0.00693952</td>\n",
       "      <td>0.00636906</td>\n",
       "      <td>0.00678045</td>\n",
       "      <td>0.00736393</td>\n",
       "      <td>0.0529846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BetaPriorPredctr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0.000       0.300       0.500       0.667  \\\n",
       "MaxiLikeliPredctr           NaN         NaN         NaN         NaN   \n",
       "DiscrtPriorPredctr  1.22983e-18   0.0280742  0.00325072   0.0303582   \n",
       "DiscrtPriorPredctr  8.19885e-19   0.0170564   0.0216698   0.0208027   \n",
       "DiscrtPriorPredctr          0.5           0           0           0   \n",
       "DiscrtPriorPredctr    0.0909115  0.00903225   0.0090073  0.00903628   \n",
       "DiscrtPriorPredctr    0.0155655  0.00980296  0.00980296  0.00980296   \n",
       "BetaPriorPredctr     0.00990099  0.00990099  0.00990099  0.00990099   \n",
       "BetaPriorPredctr    5.33447e-05   0.0128259   0.0178762   0.0141058   \n",
       "BetaPriorPredctr      0.0563485  0.00693952  0.00636906  0.00678045   \n",
       "BetaPriorPredctr            NaN         NaN         NaN         NaN   \n",
       "\n",
       "                         0.750        0.999  \n",
       "MaxiLikeliPredctr          NaN          NaN  \n",
       "DiscrtPriorPredctr   0.0142947  2.12576e-17  \n",
       "DiscrtPriorPredctr  0.00898477  3.63209e-18  \n",
       "DiscrtPriorPredctr           0        0.475  \n",
       "DiscrtPriorPredctr  0.00896276    0.0800066  \n",
       "DiscrtPriorPredctr  0.00980296    0.0151145  \n",
       "BetaPriorPredctr    0.00990099   0.00990099  \n",
       "BetaPriorPredctr     0.0107185  5.74868e-05  \n",
       "BetaPriorPredctr    0.00736393    0.0529846  \n",
       "BetaPriorPredctr           NaN          NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lk_df = pd.DataFrame(np.mean(likelihoods, axis = 2), \n",
    "             index = [printStr(r, 3) for r in true_rs], \n",
    "             columns = [str(a).split(' ', 1)[0] for a in predictors])\n",
    "lk_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10** (1 point + 1 bonus point): Now analyse the results of your experiment. Answer at least the following questions:\n",
    "\n",
    "* For each combination of predictor and $r_\\text{true}$ that gives `inf` or `nan` loss, explain why the total loss isn't finite.\n",
    "\n",
    "* What are the strengths and weaknesses of the different predictors? In a situation where you don't know anything about $r_\\text{true}$ beforehand, which priors would you recommend, and which not?\n",
    "\n",
    "* BONUS: Look at the marginal likelihoods, say something about model selection\n",
    "\n",
    "You can accompany your answer by additional code that produces tables or figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** When collapsing the **N** repeats in the 3rd dimensions into a 2 dimensional matrix, if there is a single entry in the repeat vector that is **Inf**, all values in the nominator of the average will be added to **Inf**, hence the mean of the third dimension will also be **Inf** in the 2 dimensional matrix. Since that's calculated as: $\\frac{1}{N}\\Sigma \\text{ losses}$. Infinity + something and then divided by N is still infinity. The same is true for NaNs.\n",
    "\n",
    "An entry in the repeat vector gets **Inf** in the cases of **r_hat** = 0 of **r_hat** = 1, and if a generate flip does not precisely match this probability of the coin. However, this is entirely possible, since the data is drawn from a binomial distribution, based on **r_true**. In the calculation of the loss, the log of **r_hat** = 0 or **r_hat** = 1, will result in **-Inf**; when multiplied with -1, will be **Inf** and adding the second term will be, for most values of the second term (e.g. not -infinite), still **Inf**. Hence the loss will be **Inf** for that **r_hat**.\n",
    "\n",
    "So, to restate, this occurs only if the predicted **r_hat** is either 0 or 1, *and* the generated test-data is not in accordance with that. For instance, not either only tails in the case of **r_hat** = 0; not only heads in the case of **r_hat** = 1.\n",
    "&nbsp;  \n",
    "\n",
    "**Answer 2: Predictability measure**. In answering this, we will keep in mind that we do not know anything about r_true. This means that it could be any value between 0 and 1.\n",
    "* A problem with the MaximumLikelihoodPredictor and BetaPriorPredictor is that when we predict **r_hat** to be either 0 or 1, we are likely to get a prediction that is not useful. This means using this approach is prone to sampling errors. Because only when the **r_true** is exactly 0 or 1, we can get a prediction for **r_hat** that gives **Inf** loss, but also when **r_true** is just close to 0! Since the closer it is to 0, the higher the chance that the training-set will be such that we predict **r_hat** = 0 or 1. Like, in this example we used an **r_true** of exactly 0, but you can easily make a symmetry-argument, and also verify in this code (which we did!) that if you use an r-true of 0.01 you also get **Inf**.\n",
    "* A disadvantage of the DiscretePriorPredictor is that it gives unusable predictions for one of the initialisations. As we will elaborate on a bit further, this could be avoided by (slightly) different initialisation.\n",
    "\n",
    "The predictors could give Nan or Inf as prediction, which is neither useful for prediction nor does it correspond with real-life situations. However, all predictors, expect for the MaximumLiklihoodPredictor, can be initialized with different values of **r_hat** (thus different liklihoods for tails and heads), which are able to give reasonable values (or non-infinite and nan values). Thus, the discrete and the beta predictors with the overall lowest regret’s and largest marginals across all r, would be the most suitable priors. More specifically: \n",
    "* For instance DiscretePriorPredictor 2 has the lowest marginals for almost all values of **r_true**, expect for its most extreme values, which however are very unlikely, if not, realistically impossible to have in a two sided coin (e.g. that should been only one surface). The loss and regret are only very low (and applicable) for those extreme values. Similarly, its 'Beta-equivalent', BetaPriorPredictor 3, does a better job in fitting the prior to the data when **r_true** has ideal extreme values (the marginal liklihood is higher and the loss is lower for those values), while fitting less well for other values of r. When there are strong indications that **r_true** is ultimately biased (close to 0 or 1; hence not knowing the direction of the bias), both priors can representat this belief, with the BetaPriorPrediction 3 being somewhat off and the DiscretePriorPrediction 2 being not applicable, if there are no such strong a priori indications for extreme probability of **r_true**. Therefore, the BetaPriorPredictor 3 is preferred over the DiscretePriorPredictor 2, since it can make prediction for other values of r (they do not have to match exactly the most extreme values), albeit fitting the data less well than other prior beliefs.\n",
    "* Unlike DiscretePriorPredictor 2, most models (thus expect for the homogenous ones and the 3 DiscretePriorPredictor) do not fit well when **r_true** has those ideal extreme values (the marginal likelihood is very low at those values). The BetaPriorPredictor 2 is a roughly similar to a normal distribution, hence the marginal likelihood is the highest for intermediate values of **r_true** than for the extreme values. Similarly, the DescretePriorPrediction 1 and 2 fit the data well for the intermediate values of **r_true**, with an overall better performance in terms of data fit, loss and regret DescretePriorPrediction 2 (especially if the coin is fair) and a somewhat better performance for DescretePriorPrediction 1 when it is expected that **true_r** is slightly biased towards either side (1/3 and 2/3). Theoretically and realistically those priors could be considered the most reasonable ones, if the prior belief is that the coin is fair (eventually excluding DescretePriorPrediction 1) or when there is somewhat bias involved in any direction. They are not suitable if there are strong indications for heavy bias (BetaPriorPredictor 3 would be a better choice)\n",
    "* The homogenous models (DiscretePriorPredictor 4 and 5 and BetaPriorPredictor 1) all fit the data equally bad, with very low marginal likelihood at every value of r. With respect to the model loss and marginal likelihood the DiscretePriorPredictors they tend to do better for **r_hat** having ideally extreme values of 0 and 1; DiscretePriorPredictor 4 has a ‘coarser’ distribution than 5 and tends to perform better.  This makes only sense when there is extremely limited knowledge about **r_true** whatsoever.\n",
    "* The BetaPriorPredictor 4, can be theoretically and practically problematic, since it does satisfy the constraints for being a proper Beta prior. Although its numerical representations in loss and regret are comparable to the other models, the model fit to the data cannot be assesed, since this this degenerated prior lacks its normalizaion constant. There is uncertainty whether the posterior of the BetaPriorPredictor 4 becomes a proper distribution. This prior does not perform better than the others and given its drawbacks can better be avoided.\n",
    "\n",
    "Concluding, the MaximumLiklihoodPredictor, DescretePriorPredictor 3 and BetaPriorPredictor are not recommended, while the usefulness of the other predictors depend on the belief of how strongly the coin is biased or fair, with DescretePriorPredictor 2 being the most recommended choice (unless there are strong beliefs that the coin is heavily biased, then the choice would fall on BetaPrior Prediction 3). Although if absolutely nothing is known about **r_true** (lacking any indication) the best choice might probably be the homogenous DiscretePriorPredictor 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember: Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**<br/>\n",
    "**Also remove our hints/comments from your code**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
